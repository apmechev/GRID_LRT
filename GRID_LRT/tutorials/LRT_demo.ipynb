{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID_LRT Testbed Notebook\n",
    "\n",
    "## 1. Setting up the Environment\n",
    "\n",
    "The GRID LOFAR TOOLS have several infrastructure requirements. They are as follows:\n",
    "\n",
    "1. ASTRON LOFAR staging credentials\n",
    "2. PiCaS database access\n",
    "3. Valid GRID proxy\n",
    "\n",
    "\n",
    "Here, we'll test that all of the above are enabled and work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/apmechev/software/lib/python2.6/site-packages/GRID_LRT-0.2-py2.6.egg/GRID_LRT/__init__.py\n",
      "2018-02-07 17:12:09.121059 stager_access: Parsing user credentials from /home/apmechev/.awe/Environment.cfg\n",
      "2018-02-07 17:12:09.121255 stager_access: Creating proxy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import GRID_LRT\n",
    "print(GRID_LRT.__file__)\n",
    "import subprocess\n",
    "from GRID_LRT.get_picas_credentials import picas_cred\n",
    "from GRID_LRT.Staging import stage_all_LTA\n",
    "from GRID_LRT.Staging import state_all\n",
    "from GRID_LRT.Staging import stager_access\n",
    "from GRID_LRT.Staging.srmlist import srmlist\n",
    "from GRID_LRT import Token\n",
    "pc=picas_cred()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give a confirmation of that your LOFAR ASTRON credentials were properly read:\n",
    "\n",
    "`2017-12-04 17:15:29.097902 stager_access: Parsing user credentials from /home/apmechev/.awe/Environment.cfg\n",
    "2017-12-04 17:15:29.097973 stager_access: Creating proxy`\n",
    "\n",
    "Next, we check that your PiCaS User and Database are set properly. You can also verify your password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apmechev\n",
      "sksp_unittest\n"
     ]
    }
   ],
   "source": [
    "print(pc.user)\n",
    "print(pc.database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the test srm.txt to show off our staging chops:\n",
    "\n",
    "Stage the test srm.txt file. You'll get a StageID that you can use later.\n",
    "\n",
    "## 2. Staging files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB100_uv.dppp.MS_3d78b8f1.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB101_uv.dppp.MS_acbb43a6.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB102_uv.dppp.MS_69304702.tar']\n",
      "files are on SARA\n",
      "Setting up 51 srms to stage\n",
      "staged with stageID  18748\n",
      "18748\n"
     ]
    }
   ],
   "source": [
    "test_srm_file='/home/apmechev/t/GRID_LRT/GRID_LRT/tests/srm_50_sara.txt'\n",
    "\n",
    "os.path.exists(test_srm_file)\n",
    "with open(test_srm_file,'r') as f:\n",
    "    file_contents = f.read()\n",
    "    print(file_contents.split()[0:3]) \n",
    "stageID=stage_all_LTA.main(test_srm_file) # NOTE! You (oll get two emails every time you do this!\n",
    "print(stageID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now re-run the cell below to check the current status of your staging request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scheduled\n",
      "{'18748': {'Status': 'scheduled', 'File count': '51', 'User id': '2331', 'Location': 'sara', 'Files done': '0', 'Flagged abort': 'false', 'Percent done': '0'}}\n"
     ]
    }
   ],
   "source": [
    "print(stage_all_LTA.get_stage_status(stageID)) #crashes (py2.7?)\n",
    "#The code below can also show you a more detailed status\n",
    "statuses=stager_access.get_progress()\n",
    "\n",
    "print(statuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staging request 18748 has status: scheduled\n"
     ]
    }
   ],
   "source": [
    "statuses=stage_all_LTA.get_stage_status(stageID)\n",
    "## When the staging completes, your stageID magically disappears from the database\n",
    "# Neat, huh?\n",
    "if not statuses:\n",
    "    print(\"Staging status no longer in LTA Database\") #This happens because bad programming\n",
    "else:\n",
    "    print(\"Staging request \"+str(stageID)+\" has status: \"+str(statuses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check the status of the srms two different ways (with srmls and with gfal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/apmechev/software/lib/python2.6/site-packages/GRID_LRT-0.2-py2.6.egg/GRID_LRT/Staging/state_all.pyc\n",
      "files are on SARA\n",
      "0/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB100_uv.dppp.MS_3d78b8f1.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "1/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB101_uv.dppp.MS_acbb43a6.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "2/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB102_uv.dppp.MS_69304702.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "3/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB103_uv.dppp.MS_a47a629e.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "4/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB104_uv.dppp.MS_2da7d035.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "5/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB105_uv.dppp.MS_7ad084dd.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "6/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB106_uv.dppp.MS_9c68322c.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "7/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB107_uv.dppp.MS_fd8aee0a.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "8/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB108_uv.dppp.MS_71343baa.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "9/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB109_uv.dppp.MS_74a09e93.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "10/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB110_uv.dppp.MS_4e823dc4.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "11/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB111_uv.dppp.MS_4df8f01f.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "12/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB112_uv.dppp.MS_578ec0cc.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "13/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB113_uv.dppp.MS_5153acbe.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "14/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB114_uv.dppp.MS_a7308002.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "15/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB115_uv.dppp.MS_5d55c402.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "16/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB116_uv.dppp.MS_eefee06b.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "17/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB117_uv.dppp.MS_c3512dfc.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "18/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB118_uv.dppp.MS_8a3a07db.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "19/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB119_uv.dppp.MS_87856a8f.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "20/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB120_uv.dppp.MS_41cdf876.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "21/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB121_uv.dppp.MS_51422fb4.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "22/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB122_uv.dppp.MS_725aa5e4.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "23/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB123_uv.dppp.MS_325bb43f.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "24/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB124_uv.dppp.MS_8b0e62f6.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "25/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB125_uv.dppp.MS_f3acd02d.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "26/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB126_uv.dppp.MS_40a68703.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "27/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB127_uv.dppp.MS_a1e25fba.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "28/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB128_uv.dppp.MS_21dda42b.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "29/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB129_uv.dppp.MS_ca83e7c8.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "30/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB130_uv.dppp.MS_ae9fdea1.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "31/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB131_uv.dppp.MS_bb1a4f54.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "32/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB132_uv.dppp.MS_314aff50.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "33/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB133_uv.dppp.MS_cce8f7ec.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "34/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB134_uv.dppp.MS_2034f17e.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "35/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB135_uv.dppp.MS_b100487b.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "36/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB136_uv.dppp.MS_04288d08.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "37/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB137_uv.dppp.MS_df9b4ff7.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "38/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB138_uv.dppp.MS_b5809b4f.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "39/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB139_uv.dppp.MS_b491a936.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "40/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB140_uv.dppp.MS_a99ed735.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "41/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB141_uv.dppp.MS_568c1e9a.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "42/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB142_uv.dppp.MS_2a25f26f.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "43/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB143_uv.dppp.MS_11c5b025.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "44/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB144_uv.dppp.MS_ffadacf3.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "45/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB145_uv.dppp.MS_8a0a0359.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "46/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB146_uv.dppp.MS_9f12d505.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "47/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB147_uv.dppp.MS_6a966762.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "48/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB148_uv.dppp.MS_5d73b756.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "49/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB149_uv.dppp.MS_5418b56d.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "50/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB150_uv.dppp.MS_f6fc7fc5.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "files are on SARA\n"
     ]
    }
   ],
   "source": [
    "print(state_all.__file__)\n",
    "staged_status = state_all.main(test_srm_file) #Only works for Sara and Poznan files!\n",
    "\n",
    "#You can also supress the printing of statuses\n",
    "staged_status1 = state_all.main(test_srm_file, printout=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. srm lists:\n",
    "\n",
    "A dedicated class exists to handle lists of srmfiles. This class is a child of the python 'list' class and thus has all the capabilites of a list with some bells and whistles. \n",
    "\n",
    "It contains as properties the OBSID and LTA location of the files. \n",
    "\n",
    "Additionally, it can create generators that convert the srm:// links to gsiftp:// links, as well as staging links (Ones that can be fed into the state_all.py script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L229507\n",
      "sara\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "test_srm_file='/home/apmechev/t/GRID_LRT/GRID_LRT/tests/srm_50_sara.txt'\n",
    "\n",
    "s_list=srmlist() #Empty list of srms\n",
    "\n",
    "with open(test_srm_file,'r') as f:\n",
    "    for i in f.read().split():\n",
    "        s_list.append(i)\n",
    "print(s_list.OBSID)\n",
    "print(s_list.LTA_location)\n",
    "print(len(s_list)) #len works as with a normal list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above commands show that you can load a set of srm links into a srmlist object, and it will also hold the LTA location and the OBSID. Each srmlist object can hold only one OBSID and one LTA location, and makes checks on each append:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should return Different OBSID than previous items:\n",
      "Different OBSID than previous items\n"
     ]
    }
   ],
   "source": [
    "juelich_srm=str(\"srm://lofar-srm.fz-juelich.de:8443/pnfs/\"+\n",
    "\"fz-juelich.de/data/lofar/ops/projects/lc7_012/583139/L583139_SB000_uv.MS_900c9fcf.tar\")\n",
    "\n",
    "try:\n",
    "    s_list.append(juelich_srm)\n",
    "except AttributeError as e:\n",
    "    print(\"Should return Different OBSID than previous items:\\n\"+str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create generators that transform all srm links into either gsiftp links (for use with globustools) or http links (for use with wget). Additionally gfal links can be made. These links can be used with the (old) staging scripts as well as to check the status of (sara and poznan) files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four different links for the same file:\n",
      "\n",
      "srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB100_uv.dppp.MS_3d78b8f1.tar\n",
      "\n",
      "gsiftp://gridftp.grid.sara.nl:2811/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB100_uv.dppp.MS_3d78b8f1.tar\n",
      "\n",
      "https://lofar-download.grid.sara.nl/lofigrid/SRMFifoGet.py?surl=srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB100_uv.dppp.MS_3d78b8f1.tar\n",
      "\n",
      "srm://srm.grid.sara.nl:8443/srm/managerv2?SFN=/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB100_uv.dppp.MS_3d78b8f1.tar\n"
     ]
    }
   ],
   "source": [
    "gsi_generator=s_list.gsi_links()\n",
    "\n",
    "g_list=[]\n",
    "for i in gsi_generator:\n",
    "    g_list.append(i)\n",
    "\n",
    "h_list=[]\n",
    "for i in s_list.http_links():\n",
    "    h_list.append(i)\n",
    "\n",
    "stage_list=[]\n",
    "for i in s_list.gfal_links():\n",
    "    stage_list.append(i)\n",
    "\n",
    "print(\"four different links for the same file:\")\n",
    "print(\"\")\n",
    "print(s_list[0]) \n",
    "print(\"\")\n",
    "\n",
    "print(g_list[-1]) #for some reason list is backwards??\n",
    "print(\"\")\n",
    "\n",
    "print(h_list[-1])\n",
    "print(\"\")\n",
    "\n",
    "print(stage_list[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gsiftp links are used with the globus-url-copy and uberftp -ls tools. \n",
    "\n",
    "The http links can be downloaded with wget\n",
    "\n",
    "the gfal links can be fed into the state_all script which returns the status of the files on the LTA.\n",
    "\n",
    "Finally: If you need to split your srmlist in a set of equally-sized chunks, this can be done with srmlist.slice_dicts. This is useful when creating jobs that run on multiple files at the same time (for example dppconcat, or even losoto steps!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['150', '140', '120', '130', '110', '100']\n",
      "\n",
      "d_10['140'] =\n",
      "['srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB140_uv.dppp.MS_a99ed735.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB141_uv.dppp.MS_568c1e9a.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB142_uv.dppp.MS_2a25f26f.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB143_uv.dppp.MS_11c5b025.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB144_uv.dppp.MS_ffadacf3.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB145_uv.dppp.MS_8a0a0359.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB146_uv.dppp.MS_9f12d505.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB147_uv.dppp.MS_6a966762.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB148_uv.dppp.MS_5d73b756.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB149_uv.dppp.MS_5418b56d.tar']\n",
      "\n",
      "d_10['150'] =\n",
      "['srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB150_uv.dppp.MS_f6fc7fc5.tar']\n",
      "\n",
      "type(d_10['100']) = <class 'GRID_LRT.Staging.srmlist.srmlist'>\n",
      "d_10['100'].OBSID = L229507\n",
      "['150', '100']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from GRID_LRT.Staging.srmlist import slice_dicts\n",
    "\n",
    "d_10=slice_dicts(s_list.sbn_dict())\n",
    "print(d_10.keys()) # Will show the 'names' of the chunks of 10  (the starting SB)\n",
    "print(\"\")\n",
    "\n",
    "print(\"d_10['140'] =\")\n",
    "print(d_10['140']) #10 srms here\n",
    "print(\"\")\n",
    "\n",
    "print(\"d_10['150'] =\")\n",
    "print(d_10['150']) #1 srm here\n",
    "print(\"\")\n",
    "\n",
    "print(\"type(d_10['100']) = \"+str(type(d_10['100']))) #the dict values are srmlist() themselves!\n",
    "print(\"d_10['100'].OBSID = \"+d_10['100'].OBSID)\n",
    "\n",
    "d_50=slice_dicts(s_list.sbn_dict(),50)\n",
    "print(d_50.keys()) # Will show the 'names' of the chunks of 50  (the starting SB)\\\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool can be used to batch create tokens such as in section 4b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokens! \n",
    "### 4. a) The manual way\n",
    "\n",
    "Next we'll interface with PiCaS and start making tokens for our Observation:\n",
    "\n",
    "here we need a string to link all the tokens in one Observation. We'll use the string 'demo_'+username in the sksp_dev database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual_token_ID = t_jupyter_demo_apmechev_manual\n"
     ]
    }
   ],
   "source": [
    "uname = os.environ['USER']\n",
    "th = Token.Token_Handler(t_type=\"jupyter_demo_\"+uname, uname=pc.user, pwd=pc.password, dbn='sksp_dev')\n",
    "\n",
    "#Create the overview_view (has the number of todo, done, error, running, [...] tokens)\n",
    "th.add_overview_view()\n",
    "\n",
    "#Add the satus views (By default 'todo', 'locked', 'done', 'error')\n",
    "th.add_status_views()\n",
    "\n",
    "#Manually create a token:\n",
    "manual_keys = {'manual_key':'manual_value','manual_int':1024}\n",
    "man_token_1 = th.create_token(keys=manual_keys, append=\"manual\") #will return the id of the manual token\n",
    "print('manual_token_ID = ' + man_token_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also manually create a Token with an automatic attachment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two attached files are: ['srm_at_token_create.txt', 'srm_added_later.txt']\n",
      "\n",
      "The attachemnt srm_at_token_create.txt was saved at /home/apmechev/t/GRID_LRT/GRID_LRT/tutorials/srm_at_token_create.txt\n"
     ]
    }
   ],
   "source": [
    "manual_keys = {'manual_key':'manual_value','manual_int':0}\n",
    "man_token_2 = th.create_token(keys=manual_keys, \n",
    "                            append=\"manual_with_attach\",\n",
    "                            attach=[open(test_srm_file),'srm_at_token_create.txt']) \n",
    "\n",
    "##We can also attach files after the token's been created:\n",
    "th.add_attachment(man_token_2, open(test_srm_file), 'srm_added_later.txt')\n",
    "\n",
    "#Double check that both files were attached. Returns a list of filenames:\n",
    "man_2_attachies = th.list_attachments(man_token_2)\n",
    "print(\"The two attached files are: \"+str(man_2_attachies))\n",
    "\n",
    "# We can also of course download attachments:\n",
    "saved_attach=th.get_attachment(man_token_2,man_2_attachies[0],savename=man_2_attachies[0])\n",
    "print(\"\")\n",
    "print('The attachemnt '+str(man_2_attachies[0])+\" was saved at \"+saved_attach)\n",
    "\n",
    "assert(os.path.exists(saved_attach))\n",
    "os.remove(saved_attach)\n",
    "assert(not os.path.exists(saved_attach))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also list the views and the tokens from each view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['todo', 'done', 'overview_total', 'locked', 'error']\n",
      "<class 'GRID_LRT.couchdb.client.ViewResults'>\n",
      "There are 1 'locked' tokens\n",
      "There are 1 'todo' tokens\n",
      "\n",
      "They are:\n",
      "(\"CouchDB token keys: ['key', 'id', 'value']\", 'Token ID: t_jupyter_demo_apmechev_manual_with_attach')\n"
     ]
    }
   ],
   "source": [
    "print(th.views.keys()) #the views member of th is a dictionary of views \n",
    "locked_tokens = th.list_tokens_from_view('locked')\n",
    "\n",
    "print(type(locked_tokens)) #It's not a list!!\n",
    "print(\"There are \"+str(len(locked_tokens))+\" 'locked' tokens\")\n",
    "\n",
    "\n",
    "todo_tokens = th.list_tokens_from_view('todo') \n",
    "# It's not a list because it procedurally pings CouchDB, ~generator\n",
    "#Use the help below to browse how it works!!\n",
    "##help(todo_tokens)\n",
    "\n",
    "print(\"There are \"+str(len(todo_tokens))+\" 'todo' tokens\")\n",
    "print(\"\")\n",
    "print(\"They are:\")\n",
    "for i in todo_tokens:\n",
    "    print(\"CouchDB token keys: \"+str(i.keys()),\"Token ID: \"+i.id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set all tokens in a view to a Status, say 'locked'. This automatically locks the tokens!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lock status of the token: 0.\n",
      "Scrub count of the token: 0.\n",
      "There are 1 'todo' tokens\n",
      "There are 1 'locked' tokens\n",
      "\n",
      "Setting status to locked for all todo tokens\n",
      "\n",
      "There are 0 'todo' tokens\n",
      "There are 2 'locked' tokens\n",
      "Lock status of the token: 1.\n",
      "\n",
      "Resetting the locked tokens\n",
      "Scrub count of the token: 1.\n",
      "There are 2 'todo' tokens\n",
      "There are 0 'locked' tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Lock status of the token: '+str(th.db[man_token_2]['lock'])+\".\")\n",
    "print('Scrub count of the token: '+str(th.db[man_token_2]['scrub_count'])+\".\")\n",
    "print(\"There are \"+str(len(th.list_tokens_from_view('todo')))+\" 'todo' tokens\")\n",
    "print(\"There are \"+str(len(th.list_tokens_from_view('locked')))+\" 'locked' tokens\")\n",
    "print(\"\")\n",
    "print(\"Setting status to locked for all todo tokens\")\n",
    "th.set_view_to_status(view_name='todo',status='locked') #Sets all todo tokens to \"locked\"\n",
    "\n",
    "todo_tokens = th.list_tokens_from_view('todo') \n",
    "print(\"\")\n",
    "\n",
    "print(\"There are \"+str(len(todo_tokens))+\" 'todo' tokens\")\n",
    "### No more todo tokens!\n",
    "\n",
    "\n",
    "locked_tokens = th.list_tokens_from_view('locked')\n",
    "print(\"There are \"+str(len(locked_tokens))+\" 'locked' tokens\")\n",
    "##Now they're all locked!\n",
    "\n",
    "print('Lock status of the token: '+str(th.db[man_token_2]['lock'])+\".\")\n",
    "#You can reset all tokens from a view back to 'todo'. This increments the scrub_count field\n",
    "\n",
    "\n",
    "resetted_tokens=th.reset_tokens('locked')\n",
    "print(\"\")\n",
    "print(\"Resetting the locked tokens\")\n",
    "print('Scrub count of the token: '+str(th.db[man_token_2]['scrub_count'])+\".\")\n",
    "print(\"There are \"+str(len(th.list_tokens_from_view('todo')))+\" 'todo' tokens\")\n",
    "print(\"There are \"+str(len(th.list_tokens_from_view('locked')))+\" 'locked' tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can create your own view. Views collect tokens that satisfy a certain boolean expression (where the token is referenced as 'doc'\n",
    "\n",
    "For example: \n",
    "\n",
    "The todo view satsifies: `'doc.lock ==  0 && doc.done == 0 '` \n",
    "\n",
    "The locked view satisfies: `'doc.lock > 0 && doc.done == 0 '`\n",
    "\n",
    "The done view satsifies: `'doc.status == \"done\" '`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ViewDefinition '_design/jupyter_demo_apmechev/_view/demo_view'>\n",
      "There is 1 tokens in the demo_view\n",
      "There are 3 tokens in the demo_view\n"
     ]
    }
   ],
   "source": [
    "th.add_view(v_name=\"demo_view\",cond='doc.manual_int == 0 ') #Only one of our tokens has manual_int==0\n",
    "print(th.views['demo_view']) #new view is here!\n",
    "\n",
    "assert(len(th.list_tokens_from_view('demo_view'))==1)\n",
    "print(\"There is \"+str(len(th.list_tokens_from_view('demo_view')))+\" tokens in the demo_view\")\n",
    "\n",
    "#Creating 2 more tokens for this view. If append isn't changed, the id is the same, so\n",
    "#new tokens won't be created! But you can imagine a loop will make creation easy right?\n",
    "_ = th.create_token(keys=manual_keys, \n",
    "                            append=\"manual_with_attach_1\",  \n",
    "                            attach=[open(test_srm_file),'srm_at_token_create.txt']) \n",
    "_ = th.create_token(keys=manual_keys, \n",
    "                            append=\"manual_with_attach_2\",\n",
    "                            attach=[open(test_srm_file),'srm_at_token_create.txt'])\n",
    "print(\"There are \"+str(len(th.list_tokens_from_view('demo_view')))+\" tokens in the demo_view\")\n",
    "assert(len(th.list_tokens_from_view('demo_view'))==3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can delete all tokens in this view easily!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Token t_jupyter_demo_apmechev_manual_with_attach\n",
      "Deleting Token t_jupyter_demo_apmechev_manual_with_attach_1\n",
      "Deleting Token t_jupyter_demo_apmechev_manual_with_attach_2\n",
      "There are 0 tokens in the demo_view\n",
      "Deleting Token t_jupyter_demo_apmechev_manual\n"
     ]
    }
   ],
   "source": [
    "th.delete_tokens('demo_view')\n",
    "assert(len(th.list_tokens_from_view('demo_view'))==0)\n",
    "print(\"There are \"+str(len(th.list_tokens_from_view('demo_view')))+\" tokens in the demo_view\")\n",
    "\n",
    "# You can also clear all the views from the database\n",
    "th.clear_all_views()\n",
    "# And you can remove the head document (will no longer be visible in the dropdown)\n",
    "th.purge_tokens()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the login node, you sholdn't lock tokens, that's responsibility of the launcher script. After the jobs finish, you can iterate over the 'error' view and reset the tokens if you wish. This makes re-running failed jobs easy, You just have to re-submit the jdl to the Workload Manager!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b) The automatic way!\n",
    "\n",
    "\n",
    "When you need to create tokens in bulk, you can do so using a .yaml file and a python dictionary.\n",
    "\n",
    "Now introducing Token Sets: Just an easy way to create tokens from a dictionary using a yaml file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "th = Token.Token_Handler(t_type=\"jupyter_demo_\"+uname, uname=pc.user, pwd=pc.password, dbn='sksp_unittest')\n",
    "th.add_overview_view()\n",
    "th.add_status_views()\n",
    "#Re-creating the documents we purged above\n",
    "\n",
    "ts=Token.TokenSet(th=th) #You need a Token_handler object to create tokensets (token sets can only be of one 'type')\n",
    "                         #(TokenHandler manages the authentification, views and token_type selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB140_uv.dppp.MS_a99ed735.tar',\n",
       " 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB141_uv.dppp.MS_568c1e9a.tar',\n",
       " 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB142_uv.dppp.MS_2a25f26f.tar',\n",
       " 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB143_uv.dppp.MS_11c5b025.tar',\n",
       " 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB144_uv.dppp.MS_ffadacf3.tar',\n",
       " 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB145_uv.dppp.MS_8a0a0359.tar',\n",
       " 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB146_uv.dppp.MS_9f12d505.tar',\n",
       " 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB147_uv.dppp.MS_6a966762.tar',\n",
       " 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB148_uv.dppp.MS_5d73b756.tar',\n",
       " 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB149_uv.dppp.MS_5418b56d.tar']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file='/home/apmechev/t/GRID_LRT/config/tutorial.cfg' \n",
    "#This config file contains Token and sandbox creation instructions\n",
    "\n",
    "#Remember this guy from Step 3? We'll now use him to create tokens automatically\n",
    "print(type(d_10))\n",
    "d_10.keys()\n",
    "d_10['140']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use a dictionary to automatically create a set of tokens. Each item in the dictionary will be its own token. The contents of the dict will be attached as a file (We use srm.txt: It's the list of links to download on the worker node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts.create_dict_tokens(iterable=d_10, key_name='start_SB',file_upload='srm.txt') \n",
    "#This will create tokens, making the iterable key as 'start_SB' field of each token "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when you look at the database, six tokens exist, each with a respective srm.txt file attached to it. They're in the 'todo' state since they were just created but you can change that with the 'th' object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID is t_jupyter_demo_apmechev_L345916_SB100 AND start_SB Value is 100\n",
      "Token ID is t_jupyter_demo_apmechev_L345916_SB110 AND start_SB Value is 110\n",
      "Token ID is t_jupyter_demo_apmechev_L345916_SB120 AND start_SB Value is 120\n",
      "Token ID is t_jupyter_demo_apmechev_L345916_SB130 AND start_SB Value is 130\n",
      "Token ID is t_jupyter_demo_apmechev_L345916_SB140 AND start_SB Value is 140\n",
      "Token ID is t_jupyter_demo_apmechev_L345916_SB150 AND start_SB Value is 150\n",
      "<Document 't_jupyter_demo_apmechev_L345916_SB100'@'778-8b62436504b54371ffe4398ee253a221' {'start_SB': '100', 'lock': 0, 'hostname': '', '_attachments': {'srm.txt': {'stub': True, 'length': 1230, 'digest': 'md5-S3fq6oKw3M6BQ5ADfjTLPg==', 'revpos': 778, 'content_type': 'text/plain'}}, 'scrub_count': 0, 'done': 0, 'output': '', 'type': 'jupyter_demo_apmechev'}>\n"
     ]
    }
   ],
   "source": [
    "for t in th.list_tokens_from_view('todo'):\n",
    "    print(\"Token ID is \"+t['key']+\" AND start_SB Value is \" +th.db[t['key']]['start_SB'])\n",
    "\n",
    "tokens=ts.tokens\n",
    "print(th.db[ts.tokens[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's delete these guys and try to make more complex tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "th.delete_tokens('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts=Token.TokenSet(th=th, tok_config=config_file) #Now we'll use the YAML configuration file to create more fields!\n",
    "\n",
    "#Slicing our list of srms to have one token per Dict Item\n",
    "d_1=slice_dicts(s_list.sbn_dict(),1)\n",
    "\n",
    "ts.create_dict_tokens(iterable=d_1, key_name='STARTSB',file_upload='srm.txt') #Let's make some more Tokens this time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t_jupyter_demo_apmechev_L345916_SB100',\n",
       " 't_jupyter_demo_apmechev_L345916_SB101',\n",
       " 't_jupyter_demo_apmechev_L345916_SB102',\n",
       " 't_jupyter_demo_apmechev_L345916_SB103',\n",
       " 't_jupyter_demo_apmechev_L345916_SB104',\n",
       " 't_jupyter_demo_apmechev_L345916_SB105',\n",
       " 't_jupyter_demo_apmechev_L345916_SB106',\n",
       " 't_jupyter_demo_apmechev_L345916_SB107',\n",
       " 't_jupyter_demo_apmechev_L345916_SB108',\n",
       " 't_jupyter_demo_apmechev_L345916_SB109']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can also see which tokens are in the database:\n",
    "ts.tokens[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Document 't_jupyter_demo_apmechev_L345916_SB133'@'3-16d74192d4428234f6bdf1dcd48b600c' {'status': 'queued', 'PIPELINE': 'tutorial', 'STARTSB': '133', 'lock': 0, 'hostname': '', 'RESULTS_DIR': 'gsiftp://gridftp.grid.sara.nl:2811/pnfs/grid.sara.nl/data/lofar/user/sksp/pipelines/tutorial/', 'times': {'now': 1000000}, 'LOFAR_PATH': '/cvmfs/softdrive.nl/lofar_sw/LOFAR/2.20.2-centos7', 'done': 0, 'progress': 0, '_attachments': {'srm.txt': {'stub': True, 'length': 123, 'digest': 'md5-GCNCC4Z3bdfiCevrBxZ53Q==', 'revpos': 2, 'content_type': 'text/plain'}}, 'OBSID': '', 'output': '', 'SBXloc': 'test/tutorial.tar', 'type': 'jupyter_demo_apmechev', 'scrub_count': 0}>\n"
     ]
    }
   ],
   "source": [
    "#And look at each one's fields individually. Notice there's more fields than before!\n",
    "print(ts.th.db[ts.tokens[33]])\n",
    "ts.add_keys_to_list('OBSID',s_list.OBSID)\n",
    "ts.add_keys_to_list('PIPELINE','tutorial1')\n",
    "#ts.add_attach_to_list('/home/apmechev/test/GRID_LRT/parsets/Pre-Facet-Calibrator-1.parset',name='Pre-Facet-Calibrator-1.parset')\n",
    "#These come from the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's delete these tokens for now\n",
    "ts.th.delete_tokens('todo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Sandbox\n",
    "\n",
    "Since the GRID worker nodes launch jobs in a temporary folder, you need a way to bundle and store your scripts. This is what we call a 'sandbox' and consists of:\n",
    "\n",
    "1. master.sh file -> Script that does your processing\n",
    "2. bin/ folder -> Folder with setup/teardown scripts\n",
    "3. repository folders -> if you have your scripts in a repo, they're added to this sandbox\n",
    "\n",
    "We do this so that all worker nodes download the same sandbox and use the same scripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
